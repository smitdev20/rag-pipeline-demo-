"""Chat streaming endpoints using Server-Sent Events.

Provides real-time streaming chat responses via SSE protocol.
"""

import asyncio
import uuid
from collections.abc import AsyncGenerator

from fastapi import APIRouter, Request
from fastapi.responses import StreamingResponse

from src.agent.chat_agent import get_agent_service
from src.models.schemas import ChatRequest, StreamChunk, StreamStatus

router = APIRouter(prefix="/chat", tags=["chat"])


async def generate_sse_stream(
    request: Request,
    chat_request: ChatRequest,
) -> AsyncGenerator[str | None]:
    """Generate SSE-formatted stream chunks.

    Yields JSON chunks in SSE data format. Handles client disconnection
    and errors gracefully. Includes status updates during processing.

    Args:
        request: The FastAPI request for disconnect detection.
        chat_request: The validated chat request.

    Yields:
        SSE-formatted data strings.
    """
    session_id = chat_request.session_id or str(uuid.uuid4())
    agent_service = get_agent_service()

    try:
        # First: received (delays so UI can show each status smoothly)
        recv = StreamChunk(content="", done=False, status=StreamStatus.RECEIVED)
        yield f"data: {recv.model_dump_json()}\n\n"
        await asyncio.sleep(0.4)

        # Before agent: searching (knowledge base)
        search = StreamChunk(content="", done=False, status=StreamStatus.SEARCHING)
        yield f"data: {search.model_dump_json()}\n\n"
        await asyncio.sleep(0.45)

        first_chunk = True
        async for content in agent_service.stream_response(
            message=chat_request.message,
            session_id=session_id,
        ):
            if await request.is_disconnected():
                return

            # On first content: generating, then content
            if first_chunk:
                gen = StreamChunk(content="", done=False, status=StreamStatus.GENERATING)
                yield f"data: {gen.model_dump_json()}\n\n"
                first_chunk = False

            chunk = StreamChunk(content=content, done=False)
            yield f"data: {chunk.model_dump_json()}\n\n"

        # Complete: done=true with status=complete
        done_chunk = StreamChunk(content="", done=True, status=StreamStatus.COMPLETE)
        yield f"data: {done_chunk.model_dump_json()}\n\n"

    except Exception as e:
        # Error: status=error, error=message
        err_chunk = StreamChunk(
            content="", done=True, status=StreamStatus.ERROR, error=str(e)
        )
        yield f"data: {err_chunk.model_dump_json()}\n\n"


@router.post("/stream")
async def stream_chat(request: Request, body: ChatRequest) -> StreamingResponse:
    """Stream chat responses via Server-Sent Events.

    Accepts a chat message and returns a streaming response with
    real-time tokens as they are generated by the LLM.

    Args:
        request: The incoming HTTP request.
        body: Chat request containing message and optional session_id.

    Returns:
        StreamingResponse with SSE-formatted chunks.

    Response Format:
        Status flow: received → searching → generating → complete (or error).
        Chunks: {"content": "...", "done": false, "status": "received|searching|generating"}
        Final: {"content": "", "done": true, "status": "complete"}
        Error: {"content": "", "done": true, "status": "error", "error": "message"}

    Raises:
        422: Validation error (empty message, invalid format).
        500: Internal server error during streaming.
    """
    return StreamingResponse(
        generate_sse_stream(request, body),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )
