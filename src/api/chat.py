"""Chat streaming endpoints using Server-Sent Events.

Provides real-time streaming chat responses via SSE protocol.
"""

import uuid
from collections.abc import AsyncGenerator

from fastapi import APIRouter, Request
from fastapi.responses import StreamingResponse

from src.agent.chat_agent import get_agent_service
from src.models.schemas import ChatRequest, StreamChunk, StreamStatus

router = APIRouter(prefix="/chat", tags=["chat"])


async def generate_sse_stream(
    request: Request,
    chat_request: ChatRequest,
) -> AsyncGenerator[str | None]:
    """Generate SSE-formatted stream chunks.

    Yields JSON chunks in SSE data format. Handles client disconnection
    and errors gracefully. Includes status updates during processing.

    Args:
        request: The FastAPI request for disconnect detection.
        chat_request: The validated chat request.

    Yields:
        SSE-formatted data strings.
    """
    session_id = chat_request.session_id or str(uuid.uuid4())
    agent_service = get_agent_service()

    try:
        # Send thinking status
        thinking_chunk = StreamChunk(
            content="", done=False, status=StreamStatus.THINKING
        )
        yield f"data: {thinking_chunk.model_dump_json()}\n\n"

        first_chunk = True
        async for content in agent_service.stream_response(
            message=chat_request.message,
            session_id=session_id,
        ):
            if await request.is_disconnected():
                return

            # Send generating status on first content chunk
            if first_chunk:
                generating_chunk = StreamChunk(
                    content="", done=False, status=StreamStatus.GENERATING
                )
                yield f"data: {generating_chunk.model_dump_json()}\n\n"
                first_chunk = False

            chunk = StreamChunk(content=content, done=False)
            yield f"data: {chunk.model_dump_json()}\n\n"

        # Send final done chunk
        final_chunk = StreamChunk(content="", done=True)
        yield f"data: {final_chunk.model_dump_json()}\n\n"

    except Exception as e:
        error_chunk = StreamChunk(content="", done=True, error=str(e))
        yield f"data: {error_chunk.model_dump_json()}\n\n"


@router.post("/stream")
async def stream_chat(request: Request, body: ChatRequest) -> StreamingResponse:
    """Stream chat responses via Server-Sent Events.

    Accepts a chat message and returns a streaming response with
    real-time tokens as they are generated by the LLM.

    Args:
        request: The incoming HTTP request.
        body: Chat request containing message and optional session_id.

    Returns:
        StreamingResponse with SSE-formatted chunks.

    Response Format:
        Each SSE chunk contains JSON: {"content": "...", "done": false}
        Final chunk: {"content": "", "done": true}
        Error chunk: {"content": "", "done": true, "error": "message"}

    Raises:
        422: Validation error (empty message, invalid format).
        500: Internal server error during streaming.
    """
    return StreamingResponse(
        generate_sse_stream(request, body),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )
